============================================================================================
                    ALGORITMO DI MACHINE LEARNING - SISTEMA DI RACCOMANDAZIONE AFLIX
                              ANALISI TEORICA E METRICHE DI VALUTAZIONE
============================================================================================

INDICE:
1. INTRODUZIONE AL SISTEMA
2. ALGORITMO SVD (SINGULAR VALUE DECOMPOSITION)
3. IMPLEMENTAZIONE COLLABORATIVE FILTERING
4. CLUSTERING K-MEANS
5. METRICHE DI VALUTAZIONE
6. FALLBACK CONTENT-BASED
7. CODICE COMPLETO DELL'ALGORITMO
8. ANALISI DELLE PERFORMANCE

============================================================================================
1. INTRODUZIONE AL SISTEMA
============================================================================================

Il sistema di raccomandazione AFlix utilizza un approccio ibrido basato su:

A) COLLABORATIVE FILTERING con SVD (Decomposizione ai Valori Singolari)
   - Algoritmo principale per utenti con sufficiente cronologia
   - Riduzione dimensionale della matrice utenti-film
   - Apprendimento automatico dei fattori latenti

B) CLUSTERING K-MEANS
   - Raggruppamento dei film nello spazio latente SVD
   - Identificazione di pattern nascosti nelle preferenze

C) CONTENT-BASED FILTERING (Fallback)
   - Raccomandazioni basate sui generi per nuovi utenti
   - Sistema di backup quando non ci sono dati sufficienti

============================================================================================
2. ALGORITMO SVD (SINGULAR VALUE DECOMPOSITION) - TEORIA
============================================================================================

DEFINIZIONE MATEMATICA:
La SVD decompone una matrice R (ratings) in tre matrici:
    R ≈ U × Σ × V^T

Dove:
- R: Matrice utenti×film delle valutazioni (sparsa)
- U: Matrice dei fattori latenti degli utenti
- Σ: Matrice diagonale dei valori singolari
- V^T: Matrice trasposta dei fattori latenti dei film

OBIETTIVO:
Ridurre la dimensionalità mantenendo l'informazione più significativa, permettendo:
1. Predizioni di rating per coppie utente-film non osservate
2. Gestione efficiente della sparsità dei dati
3. Scoperta di fattori latenti (generi nascosti, preferenze implicite)

PROCESSO DI DECOMPOSIZIONE:
1. Costruzione matrice sparsa R[users × movies]
2. Applicazione TruncatedSVD con k componenti (default: 50)
3. Ottenimento fattori utenti (U) e fattori film (V)
4. Predizione: rating_predetto = dot_product(user_factors, movie_factors)

VANTAGGI TEORICI:
- Riduzione rumore nei dati
- Cattura relazioni complesse non lineari
- Efficienza computazionale con matrici sparse
- Gestione automatica del problema di "cold start" parziale

============================================================================================
3. IMPLEMENTAZIONE COLLABORATIVE FILTERING
============================================================================================

ARCHITETTURA:
```
Input: Dataset (user_id, movie_id, rating, metadata)
   ↓
Preprocessing: Label Encoding + Sparse Matrix Construction
   ↓
SVD Training: Decomposizione in k=50 componenti
   ↓
Factor Learning: Estrazione user_factors e movie_factors
   ↓
Prediction: Calcolo rating predetti per nuove combinazioni
```

FORMULE CHIAVE:

1. COSTRUZIONE MATRICE SPARSA:
   R[i,j] = rating dell'utente i per il film j
   R è sparsa → molte celle vuote (non tutti gli utenti votano tutti i film)

2. DECOMPOSIZIONE SVD:
   R ≈ U_k × Σ_k × V_k^T
   dove k = numero di componenti latenti (default: 50)

3. PREDIZIONE RATING:
   predicted_rating[u,m] = Σ(i=1 to k) U[u,i] × V[m,i]
   Con clipping: clip(predicted_rating, 0.5, 5.0)

4. EXPLAINED VARIANCE:
   Misura quanto della varianza originale è spiegata dai k componenti
   explained_variance = Σ(σ_i^2) / Σ(σ_total^2)

GESTIONE EDGE CASES:
- Controllo dimensioni minime (min 2 utenti, 2 film)
- Componenti adattivi: k = min(n_components, min(users, movies) - 1)
- Fallback per dati insufficienti

============================================================================================
4. CLUSTERING K-MEANS - TEORIA E IMPLEMENTAZIONE
============================================================================================

OBIETTIVO:
Raggruppare i film nello spazio latente SVD per identificare cluster di preferenze

ALGORITMO K-MEANS:
1. Inizializzazione: k=3 centroidi casuali nello spazio 2D
2. Assegnazione: ogni film al centroide più vicino (distanza euclidea)
3. Aggiornamento: ricalcolo centroidi come media del cluster
4. Iterazione: ripeti 2-3 fino a convergenza

SPAZIO CARATTERISTICHE:
- Input: Prime 2 componenti SVD dei movie_factors
- Se solo 1 componente: aggiunta rumore gaussiano per clustering
- Metriche: Distanza euclidea nel piano latente

FORMULA DISTANZA:
d(film_i, centroid_j) = √[(x_i - cx_j)² + (y_i - cy_j)²]

UTILITÀ:
- Diversificazione raccomandazioni
- Scoperta di macro-generi nascosti
- Bilanciamento tra accuratezza e diversità

============================================================================================
5. METRICHE DI VALUTAZIONE - ANALISI TEORICA
============================================================================================

A) ROOT MEAN SQUARED ERROR (RMSE):
   RMSE = √[Σ(i=1 to n)(predicted_i - actual_i)² / n]
   
   INTERPRETAZIONE:
   - Misura l'errore quadratico medio delle predizioni
   - Penalizza maggiormente errori grandi
   - Valori più bassi = migliori predizioni
   - Unità: stessa scala dei rating (0.5-5.0)

B) MEAN ABSOLUTE ERROR (MAE):
   MAE = Σ(i=1 to n)|predicted_i - actual_i| / n
   
   INTERPRETAZIONE:
   - Errore assoluto medio
   - Meno sensibile agli outlier rispetto a RMSE
   - Interpretazione più intuitiva
   - Valore ideale: MAE < 1.0 per rating 1-5

C) EXPLAINED VARIANCE RATIO:
   EVR = Σ(λ_i) / Σ(λ_total)
   
   INTERPRETAZIONE:
   - Percentuale di varianza spiegata dai componenti SVD
   - Range: [0, 1], dove 1 = spiegazione perfetta
   - Valori tipici: 0.3-0.8 per sistemi di raccomandazione
   - Indica qualità della riduzione dimensionale

METODOLOGIA DI VALUTAZIONE:
1. Split train/test (80/20)
2. Training su train set
3. Predizione su test set
4. Calcolo metriche su predizioni vs ground truth

SOGLIE DI QUALITÀ:
- RMSE < 1.0: Eccellente
- RMSE 1.0-1.5: Buono
- MAE < 0.8: Eccellente
- EVR > 0.5: Buona riduzione dimensionale

============================================================================================
6. SISTEMA FALLBACK CONTENT-BASED
============================================================================================

ATTIVAZIONE:
Quando Collaborative Filtering non è applicabile:
- Utenti < 2 (impossibile calcolare correlazioni)
- Dati insufficienti per SVD

ALGORITMO:
1. Analisi preferenze generi utente
2. Calcolo rating medio per genere
3. Raccomandazioni basate su similarità generi

FORMULE:
preference_score[genre] = Σ(ratings_in_genre) / count(ratings_in_genre)
movie_score = Σ(preference_score[g] for g in movie.genres) / len(movie.genres)

============================================================================================
7. CODICE COMPLETO DELL'ALGORITMO
============================================================================================

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from scipy.sparse import csr_matrix
from typing import List, Dict, Any, Optional

class MLRecommendationService:
    def __init__(self):
        self.user_encoder = None
        self.movie_encoder = None
        self.user_factors = None
        self.movie_factors = None
        self.svd_model = None
        self.kmeans_model = None
        self.is_trained = False
        self.explained_variance = 0.0
        self.cluster_labels = None
        
        # Parametri modello
        self.n_components = 50  # Numero componenti SVD
        self.n_clusters = 3     # Numero cluster K-means

    def prepare_data(self) -> pd.DataFrame:
        """
        Prepara i dati dal database per il training
        
        Returns:
            DataFrame con colonne: userId, movieId, title, rating, genres
        """
        # Recupera tutti i voti dal database
        votes = list(Votazione.objects.all())
        if len(votes) < 10:
            raise ValueError("Insufficient voting data for training")
        
        # Converti in DataFrame strutturato
        data = []
        for vote in votes:
            data.append({
                'userId': str(vote.utente.id),
                'movieId': vote.film.tmdb_id,
                'title': vote.film.titolo,
                'rating': vote.valutazione,
                'genres': vote.film.genere
            })
        
        return pd.DataFrame(data)

    def train_model(self) -> Dict[str, Any]:
        """
        Addestra il modello SVD e clustering
        
        Processo:
        1. Preparazione dati e encoding
        2. Costruzione matrice sparsa
        3. Decomposizione SVD
        4. Clustering K-means nello spazio latente
        
        Returns:
            Statistiche di training
        """
        # 1. Preparazione dati
        df = self.prepare_data()
        
        # 2. Label encoding per conversione categorica→numerica
        self.user_encoder = LabelEncoder()
        self.movie_encoder = LabelEncoder()
        
        df['user_idx'] = self.user_encoder.fit_transform(df['userId'])
        df['movie_idx'] = self.movie_encoder.fit_transform(df['title'])
        
        # 3. Costruzione matrice sparsa R[users × movies]
        ratings_sparse = csr_matrix(
            (df['rating'], (df['user_idx'], df['movie_idx'])),
            shape=(df['user_idx'].nunique(), df['movie_idx'].nunique())
        )
        
        # 4. Calcolo componenti sicure per SVD
        min_dim = min(ratings_sparse.shape)
        max_components = max(1, min_dim - 1)
        safe_components = min(self.n_components, max_components)
        
        # Verifica requisiti minimi
        n_users = df['userId'].nunique()
        if n_users < 2:
            return self._train_content_based_model(df)
        
        # 5. Training SVD - Decomposizione ai valori singolari
        self.svd_model = TruncatedSVD(n_components=safe_components, random_state=42)
        self.user_factors = self.svd_model.fit_transform(ratings_sparse)
        self.movie_factors = self.svd_model.components_.T
        self.explained_variance = self.svd_model.explained_variance_ratio_.sum()
        
        # 6. Clustering K-means nello spazio latente
        if self.movie_factors.shape[0] > self.n_clusters:
            if self.movie_factors.shape[1] >= 2:
                # Usa prime 2 componenti per clustering 2D
                X = self.movie_factors[:, :2]
            else:
                # Se 1 componente, aggiungi rumore per clustering
                X = np.column_stack([
                    self.movie_factors[:, 0], 
                    np.random.normal(0, 0.1, self.movie_factors.shape[0])
                ])
            
            self.kmeans_model = KMeans(n_clusters=self.n_clusters, random_state=42)
            self.cluster_labels = self.kmeans_model.fit_predict(X)
        
        self.is_trained = True
        
        # Statistiche di training
        return {
            "total_ratings": len(df),
            "unique_users": df['userId'].nunique(),
            "unique_movies": df['movieId'].nunique(),
            "explained_variance": float(self.explained_variance),
            "n_components": self.svd_model.n_components,
            "training_status": "success"
        }

    def get_user_recommendations(self, user_id: str, top_n: int = 10) -> List[Dict]:
        """
        Genera raccomandazioni personalizzate usando SVD
        
        Processo:
        1. Recupero fattori latenti utente
        2. Calcolo prodotto scalare con fattori film
        3. Clipping e ordinamento predizioni
        
        Args:
            user_id: ID utente
            top_n: Numero raccomandazioni da restituire
            
        Returns:
            Lista ordinata raccomandazioni con rating predetti
        """
        if not self.is_trained:
            raise ValueError("Model not trained")
        
        # Verifica esistenza utente
        if user_id not in self.user_encoder.classes_:
            return self._get_popular_recommendations(top_n)
        
        # Recupera indice utente
        user_idx = self.user_encoder.transform([user_id])[0]
        
        # FORMULA PREDIZIONE: R_pred = U × V^T
        predicted_ratings = np.dot(self.movie_factors, self.user_factors[user_idx])
        predicted_ratings = np.clip(predicted_ratings, 0.5, 5.0)
        
        # Conversione in raccomandazioni ordinate
        movie_titles = self.movie_encoder.inverse_transform(np.arange(len(predicted_ratings)))
        
        recommendations = []
        for i, title in enumerate(movie_titles):
            film = Film.objects(titolo=title).first()
            if film:
                recommendations.append({
                    "title": title,
                    "predicted_rating": float(predicted_ratings[i]),
                    "tmdb_id": film.tmdb_id,
                    "genres": film.genere,
                    "cluster": int(self.cluster_labels[i]) if self.cluster_labels is not None else 0
                })
        
        # Ordinamento per rating predetto decrescente
        recommendations.sort(key=lambda x: x["predicted_rating"], reverse=True)
        return recommendations[:top_n]

    def evaluate_model(self) -> Dict[str, Any]:
        """
        Valuta le performance del modello usando train/test split
        
        Metriche calcolate:
        - RMSE: Root Mean Squared Error
        - MAE: Mean Absolute Error
        - Explained Variance Ratio
        
        Returns:
            Dizionario con metriche di valutazione
        """
        if not self.is_trained:
            return {"error": "Model not trained"}
        
        # Preparazione dati
        df = self.prepare_data()
        
        if len(df) < 15:
            return {"error": "Insufficient data for evaluation"}
        
        # Campionamento per efficienza
        sample_users = df['userId'].value_counts().head(100).index
        sample_df = df[df['userId'].isin(sample_users)].copy()
        
        # Re-encoding per valutazione
        user_encoder_eval = LabelEncoder()
        movie_encoder_eval = LabelEncoder()
        sample_df['user_idx'] = user_encoder_eval.fit_transform(sample_df['userId'])
        sample_df['movie_idx'] = movie_encoder_eval.fit_transform(sample_df['title'])
        
        # Split train/test (80/20)
        train_df, test_df = train_test_split(sample_df, test_size=0.2, random_state=42)
        
        # Costruzione matrice training
        train_matrix = csr_matrix(
            (train_df['rating'], (train_df['user_idx'], train_df['movie_idx'])),
            shape=(sample_df['user_idx'].nunique(), sample_df['movie_idx'].nunique())
        )
        
        # SVD per valutazione
        eval_components = min(30, min(train_matrix.shape) - 1)
        eval_components = max(1, eval_components)
        
        svd_eval = TruncatedSVD(n_components=eval_components, random_state=42)
        user_factors_eval = svd_eval.fit_transform(train_matrix)
        movie_factors_eval = svd_eval.components_.T
        
        # Generazione predizioni su test set
        predictions, actuals = [], []
        for _, row in test_df.iterrows():
            u, m = row['user_idx'], row['movie_idx']
            if u < user_factors_eval.shape[0] and m < movie_factors_eval.shape[0]:
                # FORMULA PREDIZIONE
                pred = np.dot(user_factors_eval[u], movie_factors_eval[m])
                predictions.append(pred)
                actuals.append(row['rating'])
        
        if len(predictions) == 0:
            return {"error": "No valid predictions generated"}
        
        # CALCOLO METRICHE
        # RMSE = √(Σ(pred - actual)² / n)
        rmse = float(np.sqrt(mean_squared_error(actuals, predictions)))
        
        # MAE = Σ|pred - actual| / n
        mae = float(mean_absolute_error(actuals, predictions))
        
        return {
            "rmse": rmse,
            "mae": mae,
            "test_samples": len(predictions),
            "train_samples": len(train_df),
            "explained_variance": float(self.explained_variance),
            "model_status": "evaluated"
        }

    def get_clustering_data(self) -> Dict[str, Any]:
        """
        Restituisce dati per visualizzazione clustering K-means
        
        Returns:
            Punti film e centroidi nello spazio 2D
        """
        if not self.is_trained or self.kmeans_model is None:
            return {"error": "Clustering model not available"}
        
        # Gestione spazio caratteristiche
        if self.movie_factors.shape[1] >= 2:
            X = self.movie_factors[:, :2]
        else:
            # Ricostruzione spazio 2D per visualizzazione
            X = np.column_stack([
                self.movie_factors[:, 0], 
                np.random.normal(0, 0.1, self.movie_factors.shape[0])
            ])
        
        return {
            "points": [
                {"x": float(X[i, 0]), "y": float(X[i, 1]), "cluster": int(self.cluster_labels[i])} 
                for i in range(len(X))
            ],
            "centroids": [
                {"x": float(center[0]), "y": float(center[1]), "cluster": i} 
                for i, center in enumerate(self.kmeans_model.cluster_centers_)
            ],
            "n_clusters": self.n_clusters
        }

# Istanza globale del servizio
ml_service = MLRecommendationService()
```

============================================================================================
8. ANALISI DELLE PERFORMANCE E COMPLESSITÀ
============================================================================================

COMPLESSITÀ COMPUTAZIONALE:

A) TRAINING SVD:
   - Tempo: O(k × min(m,n)²) dove k=componenti, m=utenti, n=film
   - Spazio: O(m×k + n×k) per memorizzare fattori latenti
   - Scalabilità: Lineare con il numero di rating

B) CLUSTERING K-MEANS:
   - Tempo: O(k × n × i) dove k=cluster, n=film, i=iterazioni
   - Spazio: O(n) per memorizzare assegnazioni cluster
   - Convergenza: Tipicamente < 10 iterazioni

C) PREDIZIONE:
   - Tempo per utente: O(k) - prodotto scalare
   - Tempo top-N: O(n log n) per ordinamento
   - Spazio: O(n) per predizioni temporanee

OTTIMIZZAZIONI IMPLEMENTATE:

1. MATRICI SPARSE (scipy.sparse.csr_matrix):
   - Riduzione memoria del 90%+ per dati sparsi
   - Operazioni ottimizzate per strutture sparse

2. DIMENSIONI ADATTIVE:
   - Componenti SVD adattati alla dimensione dati
   - Prevenzione errori con dati insufficienti

3. CAMPIONAMENTO VALUTAZIONE:
   - Limitazione a 100 utenti per evaluation
   - Mantenimento accuratezza con maggiore velocità

4. CLIPPING PREDIZIONI:
   - Range [0.5, 5.0] per realismo rating
   - Prevenzione valori fuori scala

PERFORMANCE ATTESE:

Dataset Piccolo (< 1000 rating):
- Training: < 1 secondo
- Predizione utente: < 100ms
- RMSE atteso: 0.8-1.2

Dataset Medio (1000-10000 rating):
- Training: 1-5 secondi
- Predizione utente: < 200ms
- RMSE atteso: 0.6-1.0

Dataset Grande (> 10000 rating):
- Training: 5-30 secondi
- Predizione utente: < 500ms
- RMSE atteso: 0.5-0.8

METRICHE DI QUALITÀ BENCHMARK:

ECCELLENTE:
- RMSE < 0.8
- MAE < 0.6
- Explained Variance > 0.7

BUONO:
- RMSE 0.8-1.2
- MAE 0.6-0.9
- Explained Variance 0.4-0.7

ACCETTABILE:
- RMSE 1.2-1.5
- MAE 0.9-1.2
- Explained Variance 0.2-0.4

============================================================================================

CONCLUSIONE:
Il sistema AFlix implementa un algoritmo di raccomandazione all'avanguardia che combina
la potenza del Collaborative Filtering via SVD con la robustezza del clustering K-means
e sistemi di fallback content-based. L'approccio teorico solido garantisce raccomandazioni
accurate e personalizzate, mentre l'implementazione ottimizzata assicura performance
scalabili per dataset di qualsiasi dimensione.

Le metriche RMSE e MAE forniscono valutazioni quantitative affidabili, mentre l'explained
variance ratio monitora la qualità della riduzione dimensionale. Il sistema è progettato
per adattarsi automaticamente alle caratteristiche dei dati disponibili, garantendo
sempre il miglior compromesso tra accuratezza e velocità di esecuzione.

============================================================================================